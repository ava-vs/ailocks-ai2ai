# Архитектура системы системы проверки фактов и источников в реальном времени для платформы Ailocks Ai2Ai Network и ключевые алгоритмы

Ключевая идея построения системы проверки фактов в реальном времени – модульный конвейер, разделенный на независимые этапы (микросервисы или бессерверные функции), которые могут выполняться параллельно и асинхронно. Это позволяет масштабировать нагрузку и снизить задержки: например, независимая проверка нескольких утверждений и одновременный сбор доказательств могут выполняться одновременно по сравнению с последовательной обработкой. В модели *микросервисов* каждая функция (извлечение утверждений, поиск доказательств, проверка) упаковывается в свой контейнер с собственным хранилищем и интерфейсом, обеспечивая гибкую масштабируемость и отказоустойчивость. Бессерверный подход (например, AWS Lambda) аналогично запускает функции по событию, упрощая развертывание (без управления серверами) и обеспечивая авто-масштаб, но может страдать от «cold start» при длительном бездействии. Событийно-ориентированная архитектура (Event-Driven) использует шину или брокер (RabbitMQ, Kafka, AWS SNS/SQS и т.д.), где один компонент публикует «событие» (например, распознанное утверждение), а другой – подписывается на него и реагирует. Такой подход позволяет практически не блокировать интерфейс пользователя и перераспределять задачи: компоненты подписываются на события типа “новый текст для проверки” или “новое доказательство найдено”. Сочетание всех трех подходов – микросервисов, бессерверных функций и событий – часто применяется: например, при получении сообщения от пользователя генерируется событие «наложение проверки» → запускаются безсерверные функции для поиска доказательств и анализа в отдельных контейнерах. Это обеспечивает высокую параллельность и поддерживает низкую задержку.

Устройство конвейера может быть таким (пример): входящий текст разбивается на атомарные утверждения (claim extraction), затем для каждого утверждения асинхронно запускаются модули поиска доказательств (например, web-поиск, база статей, знания графа), после чего агрегированные данные передаются в модуль оценки. В модуле оценки выполняются несколько действий: определение позиции источников (поддерживает ли источник утверждение, опровергает или нейтрален), оценка надежности каждого источника и выявление противоречий между источниками. Затем на основе этих данных вычисляется итоговый коэффициент достоверности ответа. Такой пятиэтапный конвейер (декомпозиция текста, выявление значимых заявлений, генерация запросов, сбор доказательств, верификация) описан в исследовании по инструменту Loki. При этом критично использовать параллельное выполнение: Loki демонстрирует, что весь процесс проверки может завершиться в совокупное время трёх обращений к LLM и одного веб-запроса, благодаря одновременному запуску независимых компонентов.

**Извлечение утверждений.** На первом этапе необходимо автоматически находить в тексте «проверяемые» утверждения. Этим занимаются методы NLP: классификация на основе моделей типа BERT или RoBERTa (задача *check-worthy claim detection*) позволяет отделить фактические утверждения от риторических фраз и мнений. Дополнительно можно применять генеративный подход: LLM или специально дообученные модели суммаризации превращают длинный текст в список кратких фактов или вопросов, пригодных для проверки. Например, в работе *Claim Extraction* показано, что современные LLM (GPT-4 и аналоги) или тонко настроенные модели суммаризации эффективно извлекают набор атомарных утверждений из исходного текста, зачастую превосходя старые методы на основе распознавания именованных сущностей.

**Сбор доказательств.** Для каждого выявленного утверждения система должна собрать релевантные данные: документы, новости, научные статьи, записи в базах знаний. Этот этап обычно строится как многопоточечный веб-поиск и запросы к различным API. Можно использовать: запросы к поисковику (Google/Bing/пользовательские API), специализированным новостным сервисам (NewsAPI, EventRegistry, Webz.io и др.), базам научных публикаций (Crossref, Semantic Scholar), а также знание-графам (например, Google Knowledge Graph или Wikidata). Знание-графы особенно полезны для сложных утверждений, включающих несколько сущностей и отношений. Алгоритмы ранжирования выбирают самые актуальные результаты (по релевантности, дате, доверенности источника) и вырезают из них выдержки (snippets) – фрагменты текста, содержащие нужную информацию. Важно также предобрабатывать и структурировать данные (например, выделять цитаты, ключевые таблицы). В некоторых подходах для улучшения поиска LLM генерирует уточняющие запросы или даже уточняющие вопросы к сайту.

**Алгоритмы верификации:** После получения доказательств нужны методы оценки:

* *Stance Detection (позиция источника).* Для каждого найденного источника/выдержки система определяет, поддерживает ли он, опровергает или не затрагивает проверяемое утверждение. Это может быть классификация нейронной сети (например, fine-tuning BERT с тремя классами «support/refute/neutral»). В генеративных подходах LLM получает утверждение и выдержки, а в ответ возвращает анализ или вердикт. Современные работы чаще задают LLM классифицировать «утверждение поддерживается/опровергается/не достаточно информации». Например, в обзоре по LLM сказано, что «типичным подходом является классификация утверждения на категории `поддерживает`, `опровергает` или `недостаточно информации` на основе предоставленных доказательств». Такая классификация может дополняться объяснениями: современные модели генерируют пояснения, почему они отнесли утверждение к тому или иному классу.

* *Оценка надежности источника.* Для каждого ресурса (домена) вычисляется метрика авторитетности. Факторы могут включать: «возраст» домена, известность издательства, историческую точность прошлых публикаций, цитируемость, экспертные рейтинги. Например, сервис NewsGuard вручную оценивает более 35 000 источников по 9 критериям журналистской практики (политическая независимость, прозрачность, фактологическая точность и т.д.), присваивая им финальный балл 0–100. Мы можем хранить такие рейтинги (из NewsGuard API или аналогичных) и использовать их для оценки доверия к доказательствам. Другие сигналы: алгоритмические (pagerank ссылок), «моральный вес» бренда (established news media vs блог), отзывы читателей. Система может комбинировать эти признаки в агрегированную оценку достоверности источника, которую затем учитывает при вычислении окончательной уверенности.

* *Обнаружение противоречий.* Система должна находить случаи, когда разные источники выдают конфликтующие сведения. Например, если одно доказательство говорит «Да, случилось», а другое «Нет, не случилось», это прямое противоречие. Алгоритмы могут анализировать множество источников попарно: сравнивать текстовые embeddings или задавать LLM вопрос «опровергает ли документ А документ Б?». Часто используют семантический анализ и логическую проверку. Например, алгоритмы на основе знаний (knowledge bases) выявляют противоречия через несовместимость фактов. Выявленные конфликты отражаются на понижение уверенности проверки.

* *Расчет оценки уверенности.* По результатам всей проверки вычисляется окончательный коэффициент достоверности утверждения. Методика оценки может учитывать множество факторов: количество и качество поддерживающих/опровергающих доказательств, доверие источников, найденные противоречия. Используют статистические или ML-модели: например, суммируют взвешенные голоса от каждого источника (вес – репутация источника), или подают мета-данные в логистическую модель, выдающую вероятность. Также можно применять эвристики: если подавляющее большинство авторитетных источников не подтверждают утверждение, оценка близка к 0%. При этом важно возвращать интерпретируемую метрику (например, шкала 0–100% достоверности). Системы могут генерировать подробную таблицу: для каждого источника указано «За»/«Против» и баллы, а итоговый балл компонуется из этих данных.

## Сторонние инструменты и API

Для ускорения разработки можно использовать готовые сервисы:

* **Google Fact Check Tools API.** Предоставляет публичное окно к базе проверок фактов (ClaimReview), собранной Google и партнерами. С его помощью можно искать уже проверенные утверждения (по ключевым словам или URL) и получать существующие «развенчания». Ограничения: охват только тех заявлений, которые нашли фактчекеры и пометили разметкой; бесплатен и легко интегрируется (требует API-ключ Google). Основное преимущество – доступ к агрегированным данным ведущих фактчек-служб, минус – данные достоверны только в объёме имеющейся базы.

* **NewsGuard.** Проприетарный сервис, предоставляющий экспертные рейтинги достоверности источников. Охватывает >35 000 источников, отвечающих за 95% новостного трафика в интернете. Предоставляет API и облачные датастримы «на борту». Каждому изданиям даются подробные «маркировки» (условно, «питание аналитики» – Nutrition Label). Преимущество – высокая точность и полнота рейтингов, регулярное обновление. Недостаток – коммерческий продукт (цена не раскрывается публично, Enterprise-уровень); доступ к данным даётся после переговоров. Имеет хороший API-клиент, который позволяет по домену получить численный рейтинг доверия (например, фильтрация в примерах кода).

* **NewsAPI (newsapi.org).** Популярный агрегатор новостей: возвращает статьи из тысяч источников в реальном времени. Есть бесплатный тариф (100 запросов/день, контент с задержкой \~24ч, только для тестирования) и платные планы (например, \$449/мес – до 250 000 запросов, реальный поток, 99.95% SLA). NewsAPI удобно использовать для поиска релевантных новостей: по ключевым словам или топ-новостям. К минусам можно отнести: нет полного текста статей (только заголовки и аннотации – полные тексты придется парсить отдельно), а также ограниченность бесплатной версии.

* **Diffbot.** Сервис парсинга и знания-графа: извлекает структурированные данные с любых веб-страниц (Article API для текста, Knowledge Graph API для поиска сущностей и фактов). Имеет бесплатный пакет (10 000 «кредитов», до 5 вызовов/минуту) и более продвинутые (от \$299/мес, 250k кредитов с 5 вызовами/сек). Позволяет автоматически получать контент страницы и метаданные (текст, дату, автора) из ссылок, а Knowledge Graph даёт поиск по сущностям (люди, организации, события и т.д.), что полезно при извлечении контекста. Минус – стоимость и ограниченный бесплатный доступ, плюс придется оплачивать за каждый вызов API.

* **Прочие сервисы и API:**

  * *EventRegistry* (eventregistry.org) – коммерческая платформа мониторинга новостей (>150k источников, поддержка 60+ языков), подходящая для отслеживания мировых событий и сбора новостей. Доступна через API (есть триал).
  * *Media Cloud* – открытая платформа MIT для анализа медийных сообщений (бесплатна для исследователей, API позволяет собирать новости и анализировать тренды).
  * *Крупные облачные NLP API:* Microsoft Azure Text Analytics и IBM Watson NLU. Они не специализируются на проверке фактов, но могут давать информацию об эмоциях, ключевых фразах, известности именованных сущностей в тексте. Например, Watson умеет извлекать факты и связи между сущностями, что может помочь понять контекст. BytePlus упоминает эти сервисы как примеры того, что «использование готовых AI-платформ (Google, IBM, Microsoft) упрощает внедрение без глубоких знаний AI».

**Сравнительный анализ:**

* *Google Fact Check API:* покрытие — фактически «форум» популярных фактчекеров (IFCN, крупных СМИ); точность высокая (данные из надежных источников), высокая задержка – низкая (быстрый ответ); интеграция простая, бесплатная (но нужна регистрация и API-ключ).
* *NewsGuard:* покрытие — широкий (35k сайтов); точность экспертная, задержка — мгновенная (API-вызов); стоимость — очень высокая (предназначена для корпоративных клиентов); интеграция – через API, требует ключ.
* *NewsAPI:* покрытие — тысячи новостных источников; точность – N/A (это просто сборщик новостей); задержка – очень низкая (обработки JSON fast); базовый план бесплатен, продвинутые платные (от \$449/мес).
* *Diffbot:* покрытие — практически любой сайт (сильный краулер), точность зависит от качества машинного разбора; задержка — зависит от тарифного плана; платный, оплаты по кредитам.
* *Другие:* Media Cloud/EventRegistry дают обширные архівы новостей, но тоже за деньги; Azure/Watson при прочих равных удобны для NLP-задач, но «самих по себе» не проверяют факты.

**Рекомендации:** В качестве основного набора рекомендуем сочетать несколько сервисов: Google Fact Check API (для быстрой проверки ранее разобранных заявлений) и NewsGuard (для оценки надежности источников), а также агрегатор новостей (NewsAPI или EventRegistry) для поиска свежей информации и доказательств. Diffbot может служить вторичным инструментом («истребителем» веб-контента) при необходимости извлечения деталей. IBM/Microsoft NLP API можно использовать как вспомогательный для предобработки текста и извлечения сущностей.

## Интеграция LLM и Prompt Engineering

LLM (например, GPT-4 или Claude) выступают как «мозг» финальной проверки: они синтезируют найденную информацию и формируют ответ. **Лучшие практики**: использовать LLM в режиме *retrieval-augmented generation*: подсовывать модели собранные доказательства вместе с вопросом. То есть питчить модель не просто «Проверить X», а «You are a fact-checker. Используя следующие выдержки, проверьте утверждение «…»». Из **Prompt Engineering** подходит метод *role specification* (инструктировать модель «вы – эксперт-фактчекинг»), а также структурированные ответы (например, в формате JSON). Исследования показывают, что такие трюки (например, задать формат ответа с полями “claim/evidence/verdict”) повышают согласованность ответов и упрощают парсинг .

Полезные шаблоны:

```text
System: “Вы – проверщик фактов, ваша задача – на основе предоставленных источников дать объективную оценку.”
User: “Утверждение: «<утверждение>». Источники: <вставьте выдержки или ссылки на доказательства>. 
Проанализируйте, поддерживают ли они это утверждение, и сформулируйте вывод с указанием источников.”
```

Такой формат гарантирует, что модель опирается на конкретные тексты. Также помогает *chain-of-thought*-подход: в запросе можно попросить «объясните шаги вашего рассуждения».

**Борьба с галлюцинациями и предвзятостью:** Чтобы LLM не сочинял сам факты, нужно чётко указать, что опираться можно только на предоставленные данные, а если информации не хватает – ответ «нет данных». К примеру, запросить: «Если в источниках нет упоминания, верните «Недостаточно информации»». Низкая температура (temperature≈0) также снижает творчество модели. Желательно давать LLM несколько альтернативных формулировок вопросов (prompt variants) и сравнивать ответы на согласованность. Кроме того, критично разнообразить источники, чтобы снизить уклон к одной точке зрения. При использовании LLM из известных провайдеров (GPT-4, Claude) можно также применять внутренние фильтры токсичности и непредвзятости.

В рамках *финального вывода* LLM должен выдавать справедливое резюме: описать найденные доказательства, огласить позицию («подтверждается», «не подтверждается» или «недостаточно данных»), а также сгенерировать краткий текст на естественном языке с указанием ссылок. Важно попросить модель обязательно упомянуть использованные ссылки («Согласно источникам…»), чтобы не оставалось «пустых» заявлений.

## Моделирование данных и хранение

Для хранения используем реляционную БД (например, PostgreSQL с расширением векторного поиска pgvector) или графовую базу. Примерное хранилище может выглядеть так:

```sql
-- Заявления (claims) и их история проверки
CREATE TABLE claims (
    id SERIAL PRIMARY KEY,
    text TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
-- Источники и их надежность
CREATE TABLE sources (
    id SERIAL PRIMARY KEY,
    domain TEXT UNIQUE NOT NULL,
    credibility_score FLOAT,        -- рейтинг надежности, например 0-100
    metadata JSONB                  -- дополнительная информация (страна, язык и т.д.)
);
-- Связки: какое доказательство по какому заявлению
CREATE TABLE evidences (
    id SERIAL PRIMARY KEY,
    claim_id INTEGER REFERENCES claims(id),
    source_id INTEGER REFERENCES sources(id),
    snippet TEXT,                   -- выдержка текста, подтверждающая или опровергающая
    stance VARCHAR(10),             -- 'support', 'refute', 'neutral'
    confidence FLOAT,               -- уверенность модели по этому доказательству (0-1)
    verified_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
-- Обратная связь от пользователей
CREATE TABLE feedback (
    id SERIAL PRIMARY KEY,
    claim_id INTEGER REFERENCES claims(id),
    user_id INTEGER,
    rating INTEGER,                 -- например: 1 (неверно), 0 (неизвестно), -1 (верно)
    comment TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

Здесь можно использовать векторное хранение: например, к таблице **claims** добавить колонку `embedding VECTOR` и сохранять эмбеддинги утверждений. Это позволит быстро искать похожие ранее проверенные заявления. Для графовой БД (Neo4j) можно хранить узлы «Утверждение», «Источник» и отношения «поддерживает»/«опровергает».

Такая схема покрывает основные данные: сами утверждения, все связанные доказательства и их источники с рейтингами. Историю проверок удобно вести в **evidences**, где фиксируется каждый найденный snippet и позиция. Пользовательский фидбек помогает впоследствии дообучать модели или корректировать метрики надежности.

## UX/UI презентации результатов

Интерфейс должен быть интуитивным и ненавязчивым. Обычно в чат-платформе фактчекер представлен через:

* **Индикаторы в тексте:** Например, после пользовательского сообщения система может автоматически подсветить (или иконкой пометить) ключевые утверждения, которые будут проверяться. На них можно повесить тултип или клик для показа результата.
* **Раскрывающаяся панель результатов:** После анализа рядом с сообщением появляется «баллончик» или выдвижная панель с кратким выводом («Утверждение не подтверждено») и ссылкой «Подробнее». По клику раскрывается подробная панель: список источников, выдержек и их краткая оценка. Например, идея Loki – показывать многослойный вывод: сначала общий статус, затем «углубление» до каждого факта, вплоть до цитат.
* **Всплывающие модальные окна:** При нажатии кнопки «Источники» можно открыть модальное окно с описанием каждого источника и его рейтингом надежности. Например, показывать рядом с названием издания цветной бейдж (зелёный/жёлтый/красный) по NewsGuard или доверительные символы. Исследования UX показывают, что метки типа «фактчекер оценил» эффективнее, чем «искусственный интеллект».

Визуализация доверия к источнику может быть графической: полоска прогресса или иконка «щит» с числом (0–100%). Сила доказательства (confidence) может отражаться размером или яркостью галочки/креста. Также можно дать возможность быстро сверить фрагмент: напротив каждого утверждения чат-бот выводит кнопки «☑️» / «❌», где при наведении показывается выдержка из доказательства, а при клике – ссылка. Главное – обеспечить прозрачность: пользователь должен видеть, откуда взято каждое утверждение ответа. Пример UX-образца: в системах ASR или суммаризаторах каждое слово/предложение кликабельно для проверки «на месте».

## Производительность и оптимизация

Для обеспечения реального времени (цель – ответы за 5–10 секунд) критично максимизировать параллелизм и кэширование. Как показано на примере Loki, параллелизация независимых шагов даёт заметный выигрыш: подгрузка документов, вызов LLM и анализ можно делать одновременно. Практический трюк – использовать асинхронную обработку (async/await или многопоточность): пока LLM анализирует одно утверждение, другая ветка выполняет поиск по web или в локальной базе. По готовности первых результатов можно обновить интерфейс, не дожидаясь завершения всего конвейера.

**Кэширование:** для повторяющихся запросов имеет смысл хранить результаты. Например, если пользователь часто проверяет одни и те же факты или похожие тексты, можно сохранять их в Redis или ElasticCache: и сам текст (для дедупликации) и список найденных доказательств с метками. Аналогично, кэшировать рейтинги источников (они обновляются редко) и ответы внешних API (Google Fact Check, NewsAPI) на некоторое время. Это снижает число обращений к платным сервисам и ускоряет выдачу.

**Асинхронность в UI:** сама чат-система не должна «замерзать» во время проверки. Решение: получать запрос от пользователя, сразу отвечать «Идёт проверка…» и фонитово выполнять pipeline, а результат по готовности подтягивать через веб-сокет или HTTP callback. Важно уведомить пользователя о долгой операции (показывая индикатор загрузки) и позволить прокрутить чат дальше.

Кроме того, стоит оптимизировать сам процесс: например, ограничивать глубину поиска доказательств (парсить только первые N ссылок) и контролировать число вызовов к LLM. Можно использовать легкие модели или технику distillation: если для большинства фактов достаточно простой проверки по данным, не обязательно вызывать самый мощный GPT4. Например, можно сначала попытаться проверить утверждение через поиск в библиотеке ранее проверенных фактов (closed-book), и лишь при неудаче – обращаться к GPT.

В итоге, сочетая микросервисную/бессерверную архитектуру, асинхронность и кэширование, можно обеспечить пользовательский опыт «в реальном времени» (подача результатов в течение \~5–10 секунд) при высоком качестве проверки.

**Источники:** приведены описания архитектурных подходов, примеры конвейеров, UX-рекомендации по фактичности, а также описания API и сервисов Google, NewsGuard, NewsAPI, Diffbot.

